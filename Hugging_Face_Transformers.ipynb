{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1Xj-zsofYpruVmdQrC1YIk29tyoF0epww",
      "authorship_tag": "ABX9TyOunVXpSPJoStQuM3tvTJtG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanos84/Tutorials/blob/main/Hugging_Face_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Before we start loading models from Hugging Face or working with frameworks like LangChain, Llama, or Nemotron, it is essential to check whether our system has access to a GPU. Large language models rely heavily on parallel computations, and running them on a CPU can be extremely slow or even impossible for bigger models.\n",
        "\n",
        "PyTorch provides simple tools to detect if CUDA is installed, whether the GPU is visible, and what GPU model is available. Knowing this information helps us choose the right model size and the correct configuration (such as FP16, 8-bit, or 4-bit quantization). It also helps prevent common errors like “CUDA out of memory” or “No GPU found” before they happen.\n",
        "\n",
        "The following small code block performs three basic checks:\n",
        "\n",
        "* whether CUDA is available on the machine,\n",
        "\n",
        "* what GPU model PyTorch can access,\n",
        "\n",
        "* and how many GPUs are detected.\n",
        "\n",
        "This is a standard first step before working with modern LLMs, because it ensures that the environment is correctly set up for fast inference."
      ],
      "metadata": {
        "id": "ItLpJnJk11k8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZWFysbx5e8c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
        "print(\"Number of GPUs:\", torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By running these three lines, we make sure that:\n",
        "\n",
        "* PyTorch is correctly installed,\n",
        "\n",
        "* CUDA is working,\n",
        "\n",
        "* and the system is ready to load models like Llama, Nemotron, or any Hugging Face Transformer.\n",
        "\n",
        "This avoids many common errors and helps you select the right model size and precision in the next steps of the tutorial."
      ],
      "metadata": {
        "id": "BDvq2LJM2NtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start building applications with Hugging Face and large language models such as Llama or Nemotron, we need to install the core Python libraries that will allow us to download models, preprocess data, run inference efficiently, and interact with the Hugging Face Hub.\n",
        "These libraries form the foundation of the entire workflow: they handle tokenization, dataset loading, GPU acceleration, evaluation metrics, and communication with your Hugging Face account.\n",
        "The following command installs all the essential tools we will use throughout this tutorial."
      ],
      "metadata": {
        "id": "UKP-wR032a_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate evaluate huggingface_hub"
      ],
      "metadata": {
        "id": "OkTObxjC5ui9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is what each of these packages does and why we need them:\n",
        "\n",
        "1. transformers\n",
        "\n",
        "        This is the main Hugging Face library.\n",
        "        It provides:\n",
        "        * thousands of pretrained models (Llama, Nemotron, BERT, GPT-like models…)\n",
        "        * tokenizers\n",
        "        * pipelines for text generation, classification, QA, etc.\n",
        "        This is the core tool for loading and running modern LLMs.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "2. datasets\n",
        "\n",
        "        A high-performance library for loading and manipulating datasets.\n",
        "        It supports:\n",
        "        * huge datasets (streaming, memory-efficient)\n",
        "        * many formats (CSV, JSON, Parquet, Hugging Face Hub datasets)\n",
        "        * automatic train/validation/test splits\n",
        "        Useful for training, fine-tuning, and evaluation.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "3. accelerate\n",
        "\n",
        "        A library that helps run models efficiently on:\n",
        "        * GPUs,\n",
        "        * multiple GPUs,\n",
        "        * or across distributed systems.\n",
        "        It automatically manages device placement, mixed precision (fp16, bf16), and speed optimizations.\n",
        "        We need it especially for larger models or fine-tuning.\n",
        "\n",
        "\n",
        "---\n",
        "4. evaluate\n",
        "\n",
        "        A library that provides many standard NLP metrics.  Useful when measuring model performance or comparing different model outputs.\n",
        "---\n",
        "5. huggingface_hub\n",
        "\n",
        "        It's the “connection” between your local machine and Hugging Face."
      ],
      "metadata": {
        "id": "uSDtWvK84J3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple model depolyment\n",
        "\n",
        "Before working with large models such as Llama or Nemotron, it is always a good idea to test our environment using a very small and lightweight transformer. This helps confirm that the Hugging Face transformers library is correctly installed, that pipelines work as expected, and that basic inference runs without errors.\n",
        "\n",
        "In the following example, we load a tiny sentiment-analysis model from Hugging Face (DistilBERT) and run a simple test to verify that everything is functioning. This step is like a “health check” for your environment before moving on to bigger models."
      ],
      "metadata": {
        "id": "ZC4LVyAgAR00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def test_small_transformer():\n",
        "    # Load a small transformer model for testing\n",
        "    nlp = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "    # Test the model with a sample input\n",
        "    result = nlp(\"I love using transformers library!\")\n",
        "\n",
        "    # Check if the output is as expected\n",
        "    assert result[0]['label'] == 'POSITIVE'\n",
        "    assert result[0]['score'] > 0.9\n",
        "    return result\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(test_small_transformer())\n",
        "    print(\"Small transformer test passed.\")"
      ],
      "metadata": {
        "id": "UlgYKQAT5zXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script performs a simple diagnostic to ensure that the `transformers` library and inference pipeline are running correctly. The `pipeline` function is one of the easiest ways to use pretrained models.\n",
        "It hides the complexity of tokenization, model loading, and tensor handling."
      ],
      "metadata": {
        "id": "yn9lhUzxAfH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After verifying that our environment works with a small sentiment-analysis model, we can move to another basic but very common task: text generation.\n",
        "Hugging Face provides an extremely easy way to generate text using the pipeline API.\n",
        "Here, we will use GPT-2, a lightweight autoregressive language model that can run almost anywhere (CPU or GPU).\n",
        "\n",
        "This step helps students understand how text-generation works before using more advanced models such as Llama, Nemotron, or any modern LLM."
      ],
      "metadata": {
        "id": "kHBm4jZ2Fy84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def test_text_generation():\n",
        "    # Load a small text-generation model\n",
        "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "    # Generate text from an initial prompt\n",
        "    output = generator(\"Laghouat mornings feel\", max_length=100)\n",
        "\n",
        "    return output\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(test_text_generation())\n",
        "    print(\"Text generation test passed.\")\n"
      ],
      "metadata": {
        "id": "hoo2Q2FGF0XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning a Vision Transformer (ViT) Locally\n",
        "\n",
        "In this section, we will take a small Vision Transformer (ViT) model and train it on a simple medical-image classification task.\n",
        "\n",
        "\n",
        "## Download A dataset\n",
        "\n",
        "We start by getting the  [Chest X-Ray Images (Pneumonia)](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia) from Kaggle"
      ],
      "metadata": {
        "id": "TEsNOU5eG4bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "mMoMNqgcH_Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad2e021b"
      },
      "source": [
        "!cp -r /kaggle/input/chest-xray-pneumonia /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -R /content/chest-xray-pneumonia/chest_xray/__MACOSX\n",
        "!rm -R /content/chest-xray-pneumonia/chest_xray/chest_xray"
      ],
      "metadata": {
        "id": "4UyKReUI6Yrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"imagefolder\", data_dir=\"/content/chest-xray-pneumonia/chest_xray\")"
      ],
      "metadata": {
        "id": "5OYJ4cAJIf6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We download the vision transformer `vit-base-patch16-224-in21k` which is a lightweight model trained on `ImageNet-21k` and is fast to fine-tune on a single GPU.\n",
        "\n",
        "But befor that, make sure to have the latest version of transformers:"
      ],
      "metadata": {
        "id": "hPs55lLdJKOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing step\n",
        "\n",
        "Before training a Vision Transformer (ViT), we need to convert raw images from our dataset (JPEG, PNG, etc.) into the correct numerical format expected by the model.\n",
        "\n",
        "Hugging Face provides an Image Processor (here: ViTImageProcessor) that handles:\n",
        "\n",
        "* resizing images to the model’s required size\n",
        "\n",
        "* normalizing pixel values\n",
        "\n",
        "* converting them to PyTorch tensors\n",
        "\n",
        "* packaging them with labels\n",
        "\n"
      ],
      "metadata": {
        "id": "kkRbQiRIspHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets timm torch torchvision"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5TC8IQlK8tSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "data_dir = \"/content/chest-xray-pneumonia/chest_xray\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),   # ViT expects 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # grayscale normalization\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform)\n",
        "val_dataset   = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=transform)\n",
        "test_dataset  = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=transform)\n"
      ],
      "metadata": {
        "id": "78N1FvTP8xlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT expects 224×224 images. PyTorch models expect tensors, not PIL images. So, images should be normalized (mean/std scaling) for better training.\n",
        "`datasets.ImageFolder` is a  `PyTorch` utility apllied to automatically label images based on folder names.\n",
        "\n",
        "It applies the transform to each image on the fly when accessed.\n",
        "\n",
        "We, next,  chose Vision Transformer (ViT) from Hugging Face:"
      ],
      "metadata": {
        "id": "YfbO0ZbpKSnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    num_labels=2,   # Normal vs Pneumonia\n",
        "    id2label={0: \"NORMAL\", 1: \"PNEUMONIA\"},\n",
        "    label2id={\"NORMAL\": 0, \"PNEUMONIA\": 1}\n",
        ")"
      ],
      "metadata": {
        "id": "sU0gCZLN84Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " we use `d2label` to map numeric predictions to class names for easier interpretation and  `label2id` to map class names back to integers, needed during training. The following step is to use `DataLoader` as follows:"
      ],
      "metadata": {
        "id": "YneOH3HLKsDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "cpfksdBi87nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Step\n",
        "\n",
        "We fine-tune the ViT on the  dataset using PyTorch’s training loop. First, we pass the model to the available device (the GPU)"
      ],
      "metadata": {
        "id": "ldHc8eYsLdlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "1Dsu2F7o8-lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, fix some hyperparameters (Cross entropy for binary classification and Adam for backpropagation) and we execute small a training (3epochs):"
      ],
      "metadata": {
        "id": "7vJEwN5lLs9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(3):  # adjust epochs\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "4ox-KX5tLur2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can evaluate the model on the validation dataset (which is small)"
      ],
      "metadata": {
        "id": "YizM2P5fMNX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images).logits\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")'''\n"
      ],
      "metadata": {
        "id": "vFLfBkxY_SJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform the same transformations to the testing set and we compute the test accuracy:"
      ],
      "metadata": {
        "id": "FPyKtHapohjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "\n",
        "# Path to test folder\n",
        "test_dir = \"/content/chest-xray-pneumonia/chest_xray/test\"\n",
        "\n",
        "# Transform (resize to 224x224 for ViT)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # normalize grayscale\n",
        "])\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
        "class_names = test_dataset.classes  # ['NORMAL', 'PNEUMONIA']\n",
        "\n",
        "# DataLoader\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# --- Compute Test Accuracy ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "correct, total = 0, 0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images).logits\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_acc = 100 * correct / total\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "7boSBiJT_s_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we plot 10 randomly images and we compare true and predicted labels:"
      ],
      "metadata": {
        "id": "8vS-HNwootey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Plot 10 Random Test Images with True + Predicted Labels ---\n",
        "indices = random.sample(range(len(test_dataset)), 10)\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "for i, idx in enumerate(indices):\n",
        "    image, label = test_dataset[idx]\n",
        "    # Add batch dimension and send to device\n",
        "    img_input = image.unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(img_input).logits\n",
        "        pred = torch.argmax(output, dim=1).item()\n",
        "\n",
        "    plt.subplot(2, 5, i+1)\n",
        "    plt.imshow(image.permute(1, 2, 0), cmap=\"gray\")\n",
        "    plt.title(f\"True: {class_names[label]}\\nPred: {class_names[pred]}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(f\"10 Random Test Images\\nOverall Test Accuracy: {test_acc:.2f}%\", fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "as4YJPiSBHCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Freezing layers\n",
        "\n",
        "We can choose to freeze all the layers and train only the last classification layer:"
      ],
      "metadata": {
        "id": "rUFK6twho0gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all parameters except the classification head\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the classifier head\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "rrL6tSKOpEqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also choose what layers to freeze:"
      ],
      "metadata": {
        "id": "BfN-hnk2pboH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_vit_layers(model, freeze_until: int):\n",
        "    \"\"\"\n",
        "    Freeze all encoder layers up to 'freeze_until' index.\n",
        "    Example: freeze_until=9 → freeze layers 0..9, keep 10..11 trainable.\n",
        "    \"\"\"\n",
        "    # Freeze embeddings if desired\n",
        "    for param in model.vit.embeddings.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Freeze encoder layers up to freeze_until\n",
        "    for i, layer in enumerate(model.vit.encoder.layer):\n",
        "        if i <= freeze_until:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # Always keep classifier head trainable\n",
        "    for param in model.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Example usage: freeze first 9 layers, train last 3 + classifier\n",
        "freeze_vit_layers(model, freeze_until=8)"
      ],
      "metadata": {
        "id": "eZmiUjpmpk7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# Simple Local RAG\n",
        "\n",
        "We will learn how to make an offline question-answering system that reads  PDF files and answers questions based on them. The system will load  PDFs, split them into small parts, convert these parts into vector embeddings, store them in a Chroma database, and finally use a local Mistral-7B model to answer  questions.\n",
        "\n",
        "## Required libraries\n",
        "\n",
        "First, we install necessary libraries:"
      ],
      "metadata": {
        "id": "ZWAMitJMu77V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.1.0 pypdf chromadb"
      ],
      "metadata": {
        "id": "vkKOwV3TGztW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we import the libraries that we need."
      ],
      "metadata": {
        "id": "Y1MVDyA_G35_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA   # <-- use langchain, not langchain_community\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "from langchain_community.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "I6SXo9vvHBq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Documents preprocessing\n",
        "\n",
        "The next step is to load all PDF files from a folder. Every PDF inside the folder will be processed."
      ],
      "metadata": {
        "id": "hbcwPDkEHKes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load all PDFs\n",
        "base_path = \"/content/pdfs\"\n",
        "\n",
        "docs = []\n",
        "for filename in os.listdir(base_path):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        loader = PyPDFLoader(os.path.join(base_path, filename))\n",
        "        docs.extend(loader.load())\n",
        "print(f\"Loaded {len(docs)} documents from {base_path}\")"
      ],
      "metadata": {
        "id": "kgh_pHnNHRKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After loading the documents, we split them into smaller text chunks. A chunk is simply a piece of text cut from your original document. This is necessary because language models cannot handle large text all at once."
      ],
      "metadata": {
        "id": "5Yxg71s_HUpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Split into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(f\"Split into {len(chunks)} chunks\")"
      ],
      "metadata": {
        "id": "ecR_daOOHqBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create embeddings using a small and fast local model. These embeddings are numerical vectors that represent the meaning of each chunk."
      ],
      "metadata": {
        "id": "VUYnQvXmHven"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Embeddings (local, no token required)\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "ekGIXK4nHzwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`all-MiniLM-L6-v2` is a much more modern and powerful type of model called a Transformer-based sentence embedding model. Unlike word2vec, it creates an embedding for whole sentences or paragraphs and is thus much more accurate for semantic search"
      ],
      "metadata": {
        "id": "4jmpk0keIPcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store embeddings vectors\n",
        "\n",
        "Vectors are lists of numbers that represent the meaning of text in a mathematical form. When we convert a text chunk into an embedding, we turn it into a vector such as [0.12, -0.55, 0.87, …]. These numbers allow a computer to compare meanings: if two chunks talk about the same idea, their vectors will be close to each other in this multi-dimensional space. Because computers cannot search meaning directly from text, they search through these vectors instead. This is why semantic search becomes possible only after converting text into vectors.\n",
        "\n",
        "We use Chroma because it is a fast, simple, and fully local database designed specifically to store and search these vectors. When we ask a question, Chroma quickly finds the most similar vector to the question’s embedding, which means it finds the most relevant chunk of text from the PDFs. It also stores data on disk, so we can reuse our vector store without rebuilding it every time.\n",
        "\n",
        "Chunks were divided into batches simply to avoid memory problems. If you try to insert thousands of chunks into Chroma at once, especially on a machine with limited RAM, the process can crash or slow down dramatically. By dividing the chunks into batches (for example 5000 at a time), we allow the computer to process smaller pieces safely, keeping the system stable and preventing memory overload."
      ],
      "metadata": {
        "id": "Z0J_dKE0I2G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Vector store with batching\n",
        "batch_size = 5000\n",
        "vectorstore = None\n",
        "\n",
        "for i in range(0, len(chunks), batch_size):\n",
        "    batch = chunks[i:i+batch_size]\n",
        "    if vectorstore is None:\n",
        "        vectorstore = Chroma.from_documents(batch, embeddings, persist_directory=\"./chroma_store\")\n",
        "    else:\n",
        "        vectorstore.add_documents(batch)\n",
        "\n",
        "print(\"Vector store built successfully\")"
      ],
      "metadata": {
        "id": "64tamnaAJfxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Q/A model\n",
        "\n",
        "The next step is to load the local language model. Here we use `Mistral-7B-Instruct`, which works well for question answering (not very good)."
      ],
      "metadata": {
        "id": "A12dbwScJpen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Local LLM (no token required)\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",   # lightweight model\n",
        "    device_map=\"auto\",\n",
        "    max_length=512,\n",
        "    temperature=0.1,\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=llm_pipeline)"
      ],
      "metadata": {
        "id": "C3G9i-5bJ0lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use `device_map` to choose the best available hardware, such as a GPU if one exists. The parameter `max_length` controls how long the model’s output can be, measured in tokens (small pieces of text). The parameter `temperature` controls how creative or deterministic the model is. A low value like 0.1 makes the model very focused, factual, and stable, producing almost the same answer every time. A high `temperature` would make the model more creative but also less reliable for precise question-answering."
      ],
      "metadata": {
        "id": "0sUnZGI7KZ56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain connection\n",
        "We then connect the vector database with the model using a RetrievalQA chain. This chain retrieves the most relevant chunks and gives them to the model to answer the question."
      ],
      "metadata": {
        "id": "5nu6h92AJ1lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. RetrievalQA chain\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "UxrF6EyyKER3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ask a question"
      ],
      "metadata": {
        "id": "wrjV5_eYKHUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa.run(\"What topics are covered in the folder?\"))"
      ],
      "metadata": {
        "id": "S-VbKQx7KMEi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}